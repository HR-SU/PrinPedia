# 张哲溢的工作日志

## 第一周

### 7-6

小组讨论、分工。

### 7-7

下载维基百科官方发布的历史数据，用JWPL(Java Wikipedia Library)进行解包，但结果不是很理想。

解出来为txt格式，组织形式非常类似csv，含有大量<ref/>标签，且没有显式的目录结构。

因此转向自己写爬虫，抓取维基百科的数据，组织成更符合数据库、后端要求的数据，以供使用。

### 7-8

用python写了一个简单的抓取网页内容的程序，能初步将指定网页解析成需要的数据形式，保存在txt中。

### 7-9

抓取了10个维基百科页面的数据，作为临时测试数据提交给数据库、后端。

学习pymongo库，直接对数据库进行读写。根据数据库、后端的需要，修改数据格式。

### 7-10

继续修改数据格式。对content(目录)的字符串处理有一些困难。

开始研究和考虑如何自动地不重复地爬取维基百科的词条。

发现抓取的指定网页的超链接(维基百科词条中会有非常多的指向维基百科其他词条的超链接)重复率很高。

### 7-13

针对爬虫速度慢的问题，学习异步加载，提高爬取速度。

关于自动不重复的爬取方法，由于我找到了维基百科的索引页，是根据字母排序的，可以将其作为一种保底方法使用，因此暂缓考虑这个问题。

使用了最基本的异步加载，在爬取的链接词条数较少时，比原来节省了一半以上的时间。

直接用上面说的不重复的爬取方法，一次性拿到大量用来爬取的词条，报了错，但数据库内的数据都已经创建出来了，暂时还不知道是什么原因。
