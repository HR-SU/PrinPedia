# 张哲溢的工作日志

## 第一周

### 7-6

小组讨论、分工。

### 7-7

下载维基百科官方发布的历史数据，用JWPL(Java Wikipedia Library)进行解包，但结果不是很理想。

解出来为txt格式，组织形式非常类似csv，含有大量<ref/>标签，且没有显式的目录结构。

因此转向自己写爬虫，抓取维基百科的数据，组织成更符合数据库、后端要求的数据，以供使用。

### 7-8

用python写了一个简单的抓取网页内容的程序，能初步将指定网页解析成需要的数据形式，保存在txt中。

### 7-9

抓取了10个维基百科页面的数据，作为临时测试数据提交给数据库、后端。

学习pymongo库，直接对数据库进行读写。根据数据库、后端的需要，修改数据格式。

### 7-10

继续修改数据格式。对content(目录)的字符串处理遇到了一些困难。

开始研究和考虑如何自动地不重复地爬取维基百科的词条。

发现抓取的指定网页的超链接(维基百科词条中会有非常多的指向维基百科其他词条的超链接)重复率很高。

## 第二周

### 7-13

针对爬虫速度慢的问题，学习异步加载，提高爬取速度。

关于自动不重复的爬取方法，由于我找到了维基百科的索引页，是根据字母排序的，可以将其作为一种保底方法使用，因此暂缓考虑这个问题。

英文维基索引页：https://en.wikipedia.org/wiki/Wikipedia:Contents/A%E2%80%93Z_index

使用了最基本的异步加载，在爬取的链接词条数较少时，比原来节省了一半以上的时间。

### 7-14

收到邮件，AWS educate账户申请成功，于是开始尝试将写的python爬虫脚本的当前版本部署到AWS试跑。

创建了一个linux实例，用PuTTY连接，发现一些问题：

已安装的python版本是2.7，且实例中没有MongoDB数据库。有待后续解决。

### 7-15

尝试利用维基百科的索引和列表进行不重复的爬取。分别在DOS游戏列表（英文）和诺贝尔奖获得者列表（中文）页面上进行了尝试。

链接分别如下：

https://en.wikipedia.org/wiki/Index_of_DOS_games_(A)

https://zh.wikipedia.org/wiki/%E8%AB%BE%E8%B2%9D%E7%88%BE%E6%96%87%E5%AD%B8%E7%8D%8E%E5%BE%97%E4%B8%BB%E5%88%97%E8%A1%A8

根据尝试的成果，向后端提供了一千多条英文数据和两百多条中文数据。

遇到的问题如下：

中文的索引比较复杂（英文索引主要是首字母），我暂时的做法是直接抓取选定区域的所有超链接，并不加以分辨。

爬取中文页面的时候遇到了IncompleteRead(6807 bytes read, 25961 more expected)这个问题，有待进一步处理。

### 7-16

已经能根据此前提到的英文索引页进行爬取。并发时偶尔会出现有些“协程”僵死的报错，尚在排查原因。

协助云端同学在云端运行一些简单的python脚本进行测试。

## 第三周

### 7-20

开会，讨论已有的成果的本周的计划。

根据后端同学反映的情况，修改爬虫脚本，减少一些数据中出现的bug，以及想办法避免因为网络波动导致的数据不全。

### 7-21

将修改脚本后重新爬取的测试数据提交给后端同学。

找到了可以通过维基的分类索引，深度优先地爬取不同维基词条的方法，仍在尝试。

### 7-22

开会讨论。弃用原数据获取方法，前端采用wiki组件，直接使用开始时从wiki官方数据库下载解包出来的数据。

给其他同学讲解wiki原始数据的形式与内容。

找到了一个繁简切换的开源库。给后端提供了少量简体词条数据。

### 7-23

写了两段c++程序，可以从wiki原始数据文件的正文部分中提取出目录与摘要（词条中第一段没有小标题的文本）。

不同词条的摘要有所不同，有些词条说也不上有摘要，程序对特殊情况的处理不妥当。

给后端提供了所需的少量“不同词条间的链接关系”数据。

### 7-24

提取摘要的某些特殊情况已经被克服，但程序对于一些列表页的处理仍有所不足。

原因是，我靠加粗字来定位摘要起始位置，但在文件中，右侧表格比正文出现的早，表格页的文字部分未必有加粗字，表格里却可能有，导致定位失败。

正在考虑如何把python的繁简切换和c++的文件解析结合起来，批量从原始数据库中生成数据。
